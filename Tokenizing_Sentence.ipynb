{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the text for the Tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing text and creating sequences for sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network and Machine Learning Model require number as feed.We can't feed the words. So in this notebook we will see how to convert the words into number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Some Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence=[\"My Name is Amrendra\",\n",
    "         \"I love Mango\",\n",
    "         \"Mango is my  favourite fruit\",\n",
    "         \"do you like choclate?\"\n",
    "         \"what is your favourite fruit\",\n",
    "         \"your cat\",\n",
    "         \"your favourite place\",\n",
    "         \"where you spend your time alone\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the word\n",
    "\n",
    "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can set nmber of word to be tokenize\n",
    "#The out of vocabulary (OOV) token represents words that are not in the index.\n",
    "tokenizer=Tokenizer(num_words=100,oov_token=\"<OOV>\")\n",
    "#we now call fit_on_texts() on the tokenizer to generate unique number ot each word\n",
    "tokenizer.fit_on_texts(Sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further read more about function refer:\n",
    "[see]( https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the word index\n",
    "* After tokenize the text, the tokenizer has a word index that contains key-value pairs for all the words.\n",
    "* The word is the key, and the number is the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'your': 2, 'is': 3, 'favourite': 4, 'my': 5, 'mango': 6, 'fruit': 7, 'you': 8, 'name': 9, 'amrendra': 10, 'i': 11, 'love': 12, 'do': 13, 'like': 14, 'choclate': 15, 'what': 16, 'cat': 17, 'place': 18, 'where': 19, 'spend': 20, 'time': 21, 'alone': 22}\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Get the number for a given word\n",
    "print(word_index[\"choclate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sequences for the sentences\n",
    "\n",
    "After you tokenize the words, the word index contains a unique number for each word. However, the numbers in the word index are not ordered. Words in a sentence have an order. So after tokenizing the words, the next step is to generate sequences for the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My Name is Amrendra', 'I love Mango', 'Mango is my  favourite fruit', 'do you like choclate?what is your favourite fruit', 'your cat', 'your favourite place', 'where you spend your time alone']\n",
      "[[5, 9, 3, 10], [11, 12, 6], [6, 3, 5, 4, 7], [13, 8, 14, 15, 16, 3, 2, 4, 7], [2, 17], [2, 4, 18], [19, 8, 20, 2, 21, 22]]\n"
     ]
    }
   ],
   "source": [
    "#call text_to_sequences() to convert the words in sequence\n",
    "Sequence=tokenizer.texts_to_sequences(Sentence)\n",
    "print(Sentence)\n",
    "print(Sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for the words which are not in word index\n",
    "\n",
    "Let's take a look at what happens if the sentence being sequenced contains words that are not in the word index.\n",
    "\n",
    "* When the word is not in the word_index then it will take the value of `<oov>` out of vocablary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentences1=[\"I Like Apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 14, 1]]\n"
     ]
    }
   ],
   "source": [
    "Sequences1 = tokenizer.texts_to_sequences(Sentences1)\n",
    "print(Sequences1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the sequences all the same length\n",
    "\n",
    "We can make the sequence of same length by using padding or by truncating the sentence.\n",
    "\n",
    "* Padding:- Adding zero to the sentence to make all the sentence of equal size. There are 2 types of padding.for padding we use `tf.keras.preprocessing.sequence.pad_sequences` to add zeros to the sequence to make them all the same length.\n",
    "    \n",
    "    \n",
    "    * Post:- In this type of padding the zero is added in the end of the sentence.\n",
    "    \n",
    "        [ 1 2 5 16 0 0 0 0]\n",
    "        \n",
    "    * By default padding of zero happens in the start of sentence. \n",
    "        \n",
    "        [0 0 0 0 1 2 5 16]\n",
    "        \n",
    "* Truncating means deleting or removing the word from the sentence to make all the sentence of equal size.\n",
    "\n",
    "We can optionally specify the max_length in pad_sequence. if the sequence is longer than max length it will truncated.The longer sequence to max length. By default it will truncate from beginning of the sequence. but we specify truancate from end also.\n",
    "\n",
    "if we won't provide max length, then the sequence are padded to max length of the sentence.\n",
    "\n",
    "To know more about padding and truncating refer:[see](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  5  9  3 10]\n",
      " [ 0  0  0  0  0  0 11 12  6]\n",
      " [ 0  0  0  0  6  3  5  4  7]\n",
      " [13  8 14 15 16  3  2  4  7]\n",
      " [ 0  0  0  0  0  0  0  2 17]\n",
      " [ 0  0  0  0  0  0  2  4 18]\n",
      " [ 0  0  0 19  8 20  2 21 22]]\n"
     ]
    }
   ],
   "source": [
    "Padded=pad_sequences(Sequence)\n",
    "print(Padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that padded with max length of sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding sequence with max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  5  9  3 10]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 11 12  6]\n",
      " [ 0  0  0  0  0  0  0  0  6  3  5  4  7]\n",
      " [ 0  0  0  0 13  8 14 15 16  3  2  4  7]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  2 17]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  2  4 18]\n",
      " [ 0  0  0  0  0  0  0 19  8 20  2 21 22]]\n"
     ]
    }
   ],
   "source": [
    "padded1=pad_sequences(Sequence,maxlen=13)\n",
    "print(padded1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that zero has added on all sentence sequence and make it of lenght 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding sequence with max length and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  9  3 10  0  0  0  0  0  0  0  0  0]\n",
      " [11 12  6  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 6  3  5  4  7  0  0  0  0  0  0  0  0]\n",
      " [13  8 14 15 16  3  2  4  7  0  0  0  0]\n",
      " [ 2 17  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4 18  0  0  0  0  0  0  0  0  0  0]\n",
      " [19  8 20  2 21 22  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "Pad2=pad_sequences(Sequence,maxlen=13,padding=\"post\")\n",
    "print(Pad2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,we can see that as we specify padding is post now zero are added in last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  5  9  3 10]\n",
      " [ 0  0 11 12  6]\n",
      " [ 6  3  5  4  7]\n",
      " [16  3  2  4  7]\n",
      " [ 0  0  0  2 17]\n",
      " [ 0  0  2  4 18]\n",
      " [ 8 20  2 21 22]]\n"
     ]
    }
   ],
   "source": [
    "pad3=pad_sequences(Sequence,maxlen=5)\n",
    "print(pad3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that our maxlength of sequence is 9 but when we constriant the max length to 5 some sequence get truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ End $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
