{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import important Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file\n",
    "df=pd.read_csv(\"review_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this May ALLAH For...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive evacuation orders in California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby as smoke fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id keyword location  \\\n",
       "0           0   1     NaN      NaN   \n",
       "1           1   4     NaN      NaN   \n",
       "2           2   5     NaN      NaN   \n",
       "3           3   6     NaN      NaN   \n",
       "4           4   7     NaN      NaN   \n",
       "\n",
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                          clean_text  \n",
       "0  Our Deeds are the Reason of this May ALLAH For...  \n",
       "1              Forest fire near La Ronge Sask Canada  \n",
       "2  All residents asked to shelter in place are be...  \n",
       "3     people receive evacuation orders in California  \n",
       "4  Just got sent this photo from Ruby as smoke fr...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing first five rows of dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our Deeds are the Reason of this May ALLAH Forgive us all', 'Forest fire near La Ronge Sask Canada', 'All residents asked to shelter in place are being notified by officers No other evacuation or shelter in place orders are expected', 'people receive evacuation orders in California', 'Just got sent this photo from Ruby as smoke from pours into a school']\n",
      "[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the independent variables and dependent Variables/=\n",
    "tweet=df[\"clean_text\"].tolist()\n",
    "target=df[\"target\"].tolist()\n",
    "print(tweet[0:5])\n",
    "print(target[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of tweet in training set 6090 \n",
      "Total Number of tweet in testing set 1523\n"
     ]
    }
   ],
   "source": [
    "training=int(len(df)*0.8)\n",
    "testing=int(len(df)-training)\n",
    "print(\"Total Number of tweet in training set {} \\nTotal Number of tweet in testing set {}\".format(training,testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our Deeds are the Reason of this May ALLAH Forgive us all', 'Forest fire near La Ronge Sask Canada', 'All residents asked to shelter in place are being notified by officers No other evacuation or shelter in place orders are expected', 'people receive evacuation orders in California', 'Just got sent this photo from Ruby as smoke from pours into a school']\n",
      "['Sinking Fast  Now or Never on North East Unsigned Radio listen at', 'that horrible sinking feeling when youve been at home on your phone for a while and you realise its been on G this whole time', 'Nigga car sinking but he snapping it up for fox ', 'You should delete this one its not an abbandoned nor sinking Thats the darsena of the Castello scaligero di Sirmione', 'that horrible sinking feeling when youve been at home on your phone for a while and you realise its been on G this whole time']\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data into trainset and testset in ratio(80:20)\n",
    "train=tweet[0:training] \n",
    "test=tweet[training:]\n",
    "print(train[:5])\n",
    "print(test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n",
      "[0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#Splitting the target variable in ratio(80:20)\n",
    "training_label=target[0:training]\n",
    "testing_label=target[training:]\n",
    "print(training_label[:5])\n",
    "print(testing_label[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n",
      "[0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert traingin label into numpy arrays for use with the network.\n",
    "training_label=np.array(training_label)\n",
    "testing_label=np.array(testing_label)\n",
    "print(training_label[:5])\n",
    "print(testing_label[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=1000\n",
    "embedding_dim=16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "\n",
    "tokenize=Tokenizer(num_words=vocab, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token=oov_tok,lower=True,split=' ',char_level=False)\n",
    "tokenize.fit_on_texts(train)\n",
    "\n",
    "word_index=tokenize.word_index\n",
    "sequence=tokenize.texts_to_sequences(train)\n",
    "training_pad=pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequencing the word and padding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sequences = tokenize.texts_to_sequences(test)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length, \n",
    "                               padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest fire near la <OOV> <OOV> <OOV> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "Forest fire near La Ronge Sask Canada\n"
     ]
    }
   ],
   "source": [
    "#In this we can check the padding and sequencing are working  or not.\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(training_pad[1]))\n",
    "print(train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model.\n",
    "\n",
    "* First we will create sequential layer.\n",
    "* Then we add Embedding layer.\n",
    "* We add Bidirectional GRU Layer.\n",
    "* Then we add Dense layer.\n",
    "* Later we will compile model with loss_function, activation_function, Matrics type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 16)           16000     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                9600      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 25,665\n",
      "Trainable params: 25,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_gru = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_gru.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/40\n",
      "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.6010 - accuracy: 0.6706 - val_loss: 0.5361 - val_accuracy: 0.7452\n",
      "Epoch 2/40\n",
      "6090/6090 [==============================] - 15s 2ms/sample - loss: 0.4427 - accuracy: 0.7995 - val_loss: 0.5085 - val_accuracy: 0.7518\n",
      "Epoch 3/40\n",
      "6090/6090 [==============================] - 15s 2ms/sample - loss: 0.4077 - accuracy: 0.8215 - val_loss: 0.5689 - val_accuracy: 0.7269\n",
      "Epoch 4/40\n",
      "6090/6090 [==============================] - 15s 2ms/sample - loss: 0.3908 - accuracy: 0.8317 - val_loss: 0.5239 - val_accuracy: 0.7374\n",
      "Epoch 5/40\n",
      "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.3777 - accuracy: 0.8342 - val_loss: 0.5640 - val_accuracy: 0.7295\n",
      "Epoch 6/40\n",
      "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.3689 - accuracy: 0.8401 - val_loss: 0.5559 - val_accuracy: 0.7360\n",
      "Epoch 7/40\n",
      "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.3559 - accuracy: 0.8450 - val_loss: 0.5665 - val_accuracy: 0.7315\n",
      "Epoch 8/40\n",
      "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.3433 - accuracy: 0.8540 - val_loss: 0.5782 - val_accuracy: 0.7308\n",
      "Epoch 9/40\n",
      "6090/6090 [==============================] - 17s 3ms/sample - loss: 0.3300 - accuracy: 0.8583 - val_loss: 0.5988 - val_accuracy: 0.7282\n",
      "Epoch 10/40\n",
      "6090/6090 [==============================] - 20s 3ms/sample - loss: 0.3203 - accuracy: 0.8658 - val_loss: 0.6314 - val_accuracy: 0.7091\n",
      "Epoch 11/40\n",
      "6090/6090 [==============================] - 15s 3ms/sample - loss: 0.3138 - accuracy: 0.8678 - val_loss: 0.6366 - val_accuracy: 0.7229\n",
      "Epoch 12/40\n",
      "6090/6090 [==============================] - 17s 3ms/sample - loss: 0.3061 - accuracy: 0.8693 - val_loss: 0.6778 - val_accuracy: 0.6960\n",
      "Epoch 13/40\n",
      "6090/6090 [==============================] - 15s 2ms/sample - loss: 0.2990 - accuracy: 0.8775 - val_loss: 0.7013 - val_accuracy: 0.7065\n",
      "Epoch 14/40\n",
      "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.2884 - accuracy: 0.8801 - val_loss: 0.7017 - val_accuracy: 0.7137\n",
      "Epoch 15/40\n",
      "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.2832 - accuracy: 0.8854 - val_loss: 0.7331 - val_accuracy: 0.7163\n",
      "Epoch 16/40\n",
      "6090/6090 [==============================] - 15s 3ms/sample - loss: 0.2759 - accuracy: 0.8921 - val_loss: 0.7318 - val_accuracy: 0.7124\n",
      "Epoch 17/40\n",
      "6090/6090 [==============================] - 15s 3ms/sample - loss: 0.2687 - accuracy: 0.8915 - val_loss: 0.7605 - val_accuracy: 0.7045\n",
      "Epoch 18/40\n",
      "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.2631 - accuracy: 0.8962 - val_loss: 0.8035 - val_accuracy: 0.7039\n",
      "Epoch 19/40\n",
      "6090/6090 [==============================] - 15s 3ms/sample - loss: 0.2577 - accuracy: 0.8952 - val_loss: 0.8011 - val_accuracy: 0.7118\n",
      "Epoch 20/40\n",
      "6090/6090 [==============================] - 15s 3ms/sample - loss: 0.2571 - accuracy: 0.8956 - val_loss: 0.8010 - val_accuracy: 0.7098\n",
      "Epoch 21/40\n",
      "6090/6090 [==============================] - 17s 3ms/sample - loss: 0.2502 - accuracy: 0.9016 - val_loss: 0.8106 - val_accuracy: 0.7098\n",
      "Epoch 22/40\n",
      "6090/6090 [==============================] - 15s 3ms/sample - loss: 0.2421 - accuracy: 0.9062 - val_loss: 0.8168 - val_accuracy: 0.6973\n",
      "Epoch 23/40\n",
      "6090/6090 [==============================] - 14s 2ms/sample - loss: 0.2382 - accuracy: 0.9053 - val_loss: 0.8886 - val_accuracy: 0.7085\n",
      "Epoch 24/40\n",
      "6090/6090 [==============================] - 15s 2ms/sample - loss: 0.2367 - accuracy: 0.9059 - val_loss: 0.8877 - val_accuracy: 0.6993\n",
      "Epoch 25/40\n",
      "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.2316 - accuracy: 0.9074 - val_loss: 0.8550 - val_accuracy: 0.6960\n",
      "Epoch 26/40\n",
      "6090/6090 [==============================] - 20s 3ms/sample - loss: 0.2259 - accuracy: 0.9140 - val_loss: 0.9237 - val_accuracy: 0.6999\n",
      "Epoch 27/40\n",
      "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.2174 - accuracy: 0.9176 - val_loss: 0.9217 - val_accuracy: 0.6973\n",
      "Epoch 28/40\n",
      "6090/6090 [==============================] - 20s 3ms/sample - loss: 0.2175 - accuracy: 0.9166 - val_loss: 0.9263 - val_accuracy: 0.7012\n",
      "Epoch 29/40\n",
      "6090/6090 [==============================] - 17s 3ms/sample - loss: 0.2098 - accuracy: 0.9184 - val_loss: 0.9934 - val_accuracy: 0.7045\n",
      "Epoch 30/40\n",
      "6090/6090 [==============================] - 17s 3ms/sample - loss: 0.2044 - accuracy: 0.9210 - val_loss: 1.0270 - val_accuracy: 0.7052\n",
      "Epoch 31/40\n",
      "6090/6090 [==============================] - 17s 3ms/sample - loss: 0.1959 - accuracy: 0.9240 - val_loss: 1.0291 - val_accuracy: 0.6973\n",
      "Epoch 32/40\n",
      "6090/6090 [==============================] - 26s 4ms/sample - loss: 0.1905 - accuracy: 0.9259 - val_loss: 1.1187 - val_accuracy: 0.6999\n",
      "Epoch 33/40\n",
      "6090/6090 [==============================] - 24s 4ms/sample - loss: 0.1868 - accuracy: 0.9299 - val_loss: 1.0462 - val_accuracy: 0.6881\n",
      "Epoch 34/40\n",
      "3840/6090 [=================>............] - ETA: 6s - loss: 0.1765 - accuracy: 0.9323"
     ]
    }
   ],
   "source": [
    "num_epoch =40\n",
    "history=model_gru.fit(training_pad, training_label, epochs=num_epoch, validation_data=(testing_padded, testing_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting loss and accuracy for training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ END $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
